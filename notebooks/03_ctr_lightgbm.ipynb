{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c1e24e5-7c89-4a4e-8ab8-d454e963e89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8771dce1-81f3-48a1-9c55-98af3b189507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# add parent directory to Python path\n",
    "parent_dir = Path.cwd().parent\n",
    "sys.path.insert(0, str(parent_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5658346-b816-4cfb-9dff-4b1dabb8b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Riff/ctr-env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/Riff/ctr-env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/Riff/ctr-env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/Riff/ctr-env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/Riff/ctr-env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/Riff/ctr-env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tval's auc: 0.770314\tval's binary_logloss: 0.481546\n",
      "[100]\tval's auc: 0.781733\tval's binary_logloss: 0.4655\n",
      "[150]\tval's auc: 0.786715\tval's binary_logloss: 0.459682\n",
      "[200]\tval's auc: 0.78952\tval's binary_logloss: 0.456697\n",
      "[250]\tval's auc: 0.790719\tval's binary_logloss: 0.455415\n",
      "[300]\tval's auc: 0.791593\tval's binary_logloss: 0.454493\n",
      "[350]\tval's auc: 0.792305\tval's binary_logloss: 0.453761\n",
      "[400]\tval's auc: 0.792823\tval's binary_logloss: 0.453216\n",
      "[450]\tval's auc: 0.793256\tval's binary_logloss: 0.452771\n",
      "[500]\tval's auc: 0.793638\tval's binary_logloss: 0.452361\n",
      "[550]\tval's auc: 0.793972\tval's binary_logloss: 0.451997\n",
      "[600]\tval's auc: 0.794226\tval's binary_logloss: 0.451701\n",
      "[650]\tval's auc: 0.794442\tval's binary_logloss: 0.451452\n",
      "[700]\tval's auc: 0.79468\tval's binary_logloss: 0.451176\n",
      "[750]\tval's auc: 0.794884\tval's binary_logloss: 0.450949\n",
      "[800]\tval's auc: 0.795093\tval's binary_logloss: 0.45073\n",
      "[850]\tval's auc: 0.795251\tval's binary_logloss: 0.45054\n",
      "[900]\tval's auc: 0.795391\tval's binary_logloss: 0.450371\n",
      "[950]\tval's auc: 0.795515\tval's binary_logloss: 0.45022\n",
      "[1000]\tval's auc: 0.795582\tval's binary_logloss: 0.45012\n",
      "[1050]\tval's auc: 0.795678\tval's binary_logloss: 0.450006\n",
      "[1100]\tval's auc: 0.79574\tval's binary_logloss: 0.449913\n",
      "[1150]\tval's auc: 0.795782\tval's binary_logloss: 0.449843\n",
      "[1200]\tval's auc: 0.795889\tval's binary_logloss: 0.449727\n",
      "[1250]\tval's auc: 0.795947\tval's binary_logloss: 0.449653\n",
      "[1300]\tval's auc: 0.796037\tval's binary_logloss: 0.449554\n",
      "[1350]\tval's auc: 0.796072\tval's binary_logloss: 0.4495\n",
      "[1400]\tval's auc: 0.79609\tval's binary_logloss: 0.449467\n",
      "Early stopping, best iteration is:\n",
      "[1385]\tval's auc: 0.796094\tval's binary_logloss: 0.449466\n",
      "{'auc': 0.7960939346273013, 'logloss': 0.4494664852873173, 'pr_auc': 0.5868706172545186, 'brier': 0.14532282898022408, 'ctr_mean_true': 0.25039333333333336, 'ctr_mean_pred': 0.2499241484779464}\n",
      "ECE: 0.006734039960116382\n",
      "{'k': 0.01, 'n_top': 4500, 'ctr_top': 0.9006666666666666, 'ctr_all': 0.25039333333333336, 'lift': 3.5970073750632188}\n",
      "{'k': 0.05, 'n_top': 22500, 'ctr_top': 0.7841333333333333, 'ctr_all': 0.25039333333333336, 'lift': 3.1316062727974527}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import src.data as data \n",
    "import src.features as features\n",
    "import importlib\n",
    "import src.metrics as metrics \n",
    "\n",
    "#Reload all the modules to make sure the changes to the dependency files are always live.\n",
    "# importlib.reload(data)\n",
    "# importlib.reload(features)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "# --------------------------\n",
    "# 1) Load Criteo train.txt (no header)\n",
    "# label + 13 numerical (I1..I13) + 26 categorical (C1..C26)\n",
    "# --------------------------\n",
    "\n",
    "nrows = 5000000           # <-- set None for full file (can be huge)\n",
    "seeds = [11]\n",
    "models = []\n",
    "\n",
    "for seed in seeds:\n",
    "    (num_features, \n",
    "     cat_features, \n",
    "     columns, \n",
    "     X_train_raw, \n",
    "     X_val_raw, \n",
    "     X_test_raw, \n",
    "     y_train, \n",
    "     y_val, \n",
    "     y_test) = data.load_data(\n",
    "        data_path = \"../data/criteo/train.txt\", data_size = nrows, train_eval_random_state = seed)\n",
    "    \n",
    "    # --------------------------\n",
    "    # 2) Label-encode categoricals (fit on TRAIN only)\n",
    "    #    Unseen categories in val -> -1\n",
    "    # --------------------------\n",
    "    # encoders = {}\n",
    "    # for c in cat_features:\n",
    "    #     uniq = X_train[c].unique()\n",
    "    #     encoders[c] = {v: i for i, v in enumerate(uniq)}\n",
    "    #     X_train[c] = X_train[c].map(encoders[c]).astype(np.int32)\n",
    "    #     X_val[c] = X_val[c].map(encoders[c]).fillna(-1).astype(np.int32)\n",
    "\n",
    "\n",
    "    # 3) Add numeric engineered features\n",
    "    X_train_raw, X_val_raw, X_test_raw, num_eng_cols = features.add_num_log_and_missing(\n",
    "        X_train_raw, X_val_raw, X_test_raw, num_features\n",
    "    )\n",
    "    \n",
    "    # 4.1) Add categorical frequency features (uses TRAIN distribution only)\n",
    "    X_train_raw, X_val_raw, X_test_raw, freq_cols = features.add_cat_frequency(\n",
    "        X_train_raw, X_val_raw, X_test_raw, cat_features\n",
    "    )\n",
    "\n",
    "    # 4.2) Add categorical log frequency features (uses TRAIN distribution only)\n",
    "    X_train_raw, X_val_raw, X_test_raw, freq_cols = features.add_log_freq(\n",
    "        X_train_raw, X_val_raw, X_test_raw, cat_features\n",
    "    )\n",
    "\n",
    "    # 5) Fill missing values.\n",
    "    data.fill_missing_values(X_train_raw, X_val_raw, X_test_raw, num_features, cat_features)\n",
    "    X_train = X_train_raw\n",
    "    X_val = X_val_raw\n",
    "    X_test = X_test_raw\n",
    "\n",
    "    # 6) Convert cat_features to hashes\n",
    "    features.convert_cat_to_hash(X_train, cat_features)\n",
    "    features.convert_cat_to_hash(X_val, cat_features)\n",
    "\n",
    "    # 7) Feature crossing on already hashed features\n",
    "    X_train, X_val, X_test, I11_bin_col, I11_edges = features.add_quantile_bin(X_train, X_val, X_test, \"I11\", n_bins=64)\n",
    "    HASH_BINS = 1 << 20  # 1,048,576\n",
    "    \n",
    "    # C10 × C17\n",
    "    X_train, X_val, X_test, cross1 = features.add_hashed_cross(\n",
    "        X_train, X_val, X_test,\n",
    "        \"C10\", \"C17\",\n",
    "        new_name=\"C10xC17\",\n",
    "        num_bins=HASH_BINS\n",
    "    )\n",
    "    \n",
    "    # C10 × I11_bin\n",
    "    X_train, X_val, X_test, cross2 = features.add_hashed_cross(\n",
    "        X_train, X_val, X_test,\n",
    "        \"C10\", I11_bin_col,\n",
    "        new_name=\"C10xI11bin\",\n",
    "        num_bins=HASH_BINS\n",
    "    )\n",
    "    \n",
    "    # --------------------------\n",
    "    # 8) LightGBM datasets\n",
    "    # --------------------------\n",
    "    new_cat_cols = [cross1, cross2, I11_bin_col]     # treat bins as categorical\n",
    "    new_num_cols = freq_cols                      # numeric\n",
    "    \n",
    "    feature_cols = list(X_train.columns)  # or explicitly build: num_eng_cols + cat_cols + freq_cols + logfreq_cols + [I11_bin_col, cross1, cross2]\n",
    "    all_cat_features = cat_features + new_cat_cols\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train, categorical_feature=all_cat_features, free_raw_data=False)\n",
    "    dval = lgb.Dataset(X_val, label=y_val, categorical_feature=all_cat_features, reference=dtrain, free_raw_data=False)\n",
    "    \n",
    "    # --------------------------\n",
    "    # 9) Minimal params for CTR\n",
    "    # --------------------------\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": [\"auc\", \"binary_logloss\"],\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"num_leaves\": 255,\n",
    "        \"max_depth\": 12,\n",
    "        \"min_data_in_leaf\": 300,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"lambda_l2\": 10,\n",
    "        \"verbosity\": -1,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "\n",
    "    # --------------------------\n",
    "    # 10) Train with early stopping\n",
    "    # --------------------------\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[dval],\n",
    "        valid_names=[\"val\"],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "            lgb.log_evaluation(period=50),\n",
    "        ],\n",
    "    )\n",
    "    models.append(model)\n",
    "\n",
    "    # --------------------------\n",
    "    # 11) Evaluate\n",
    "    # --------------------------\n",
    "    p_val = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    print(metrics.metrics_basic(y_val, p_val))\n",
    "    \n",
    "    ece, ece_table = metrics.expected_calibration_error(y_val, p_val, n_bins=15)\n",
    "    print(\"ECE:\", ece)\n",
    "\n",
    "    print(metrics.topk_lift(y_val, p_val, k=0.01))   # top 1%\n",
    "    print(metrics.topk_lift(y_val, p_val, k=0.05))   # top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e63bd-80d3-4712-b084-a45057f507c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_columns', None):\n",
    "    print(X_train.head(10))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41c8b3fa-106e-4dd0-8356-4868df290f70",
   "metadata": {},
   "source": [
    "nrows = 2000000 Train eval test split 0.8, 0.1, 0.1 seed = 10\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": [\"auc\", \"binary_logloss\"],\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 255,\n",
    "    \"max_depth\": 12,\n",
    "    \"min_data_in_leaf\": 300,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"lambda_l2\": 10,\n",
    "    \"verbosity\": -1,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "val's auc: 0.792685\tval's binary_logloss: 0.452031\n",
    "{'auc': 0.7926847438327466, 'logloss': 0.4520309908152995, 'pr_auc': 0.586358610353339, 'brier': 0.14611620300502454, 'ctr_mean_true': 0.25104444444444446, 'ctr_mean_pred': 0.25024165900316586}\n",
    "ECE: 0.006047129846060854\n",
    "{'k': 0.01, 'n_top': 1800, 'ctr_top': 0.8938888888888888, 'ctr_all': 0.25104444444444446, 'lift': 3.560679826502597}\n",
    "{'k': 0.05, 'n_top': 9000, 'ctr_top': 0.7903333333333333, 'ctr_all': 0.25104444444444446, 'lift': 3.148180932991047}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c1a99caa-b3d4-43ea-9da4-11e04b44e75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.7946398988802232, 'logloss': 0.4517447045191126, 'pr_auc': 0.5831389395321789, 'brier': 0.14628356984096172, 'ctr_mean_true': 0.25125, 'ctr_mean_pred': 0.24875493089817768}\n",
      "ECE: 0.00886980180107436\n",
      "{'k': 0.01, 'n_top': 5000, 'ctr_top': 0.8912, 'ctr_all': 0.25125, 'lift': 3.547064676616902}\n",
      "{'k': 0.05, 'n_top': 25000, 'ctr_top': 0.77744, 'ctr_all': 0.25125, 'lift': 3.0942885572139183}\n"
     ]
    }
   ],
   "source": [
    "# Run the model on test set.\n",
    "#print(X_test.head(1))\n",
    "features.convert_cat_to_hash(X_test, cat_features)\n",
    "#print(X_test.head(1))\n",
    "\n",
    "p_test = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "print(metrics.metrics_basic(y_test, p_test))\n",
    "\n",
    "ece, ece_table = metrics.expected_calibration_error(y_test, p_test, n_bins=15)\n",
    "print(\"ECE:\", ece)\n",
    "\n",
    "print(metrics.topk_lift(y_test, p_test, k=0.01))   # top 1%\n",
    "print(metrics.topk_lift(y_test, p_test, k=0.05))   # top 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9eef483b-de84-41e8-ad74-f559be15b7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x1d3097e50>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the last model to disk\n",
    "model.save_model(\"../models/lgb/lgb_ctr_model.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68bd5b7f-278b-4ba3-bbec-831f2f78a8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC     : 0.7962 ± nan\n",
      "loglosses: 0.4501 ± nan\n",
      "briers  : 0.1457 ± nan\n",
      "eces    : 0.0039 ± nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Riff/ctr-env/lib/python3.9/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/Riff/ctr-env/lib/python3.9/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "aucs = []\n",
    "loglosses = []\n",
    "briers = []\n",
    "eces = []\n",
    "\n",
    "for model in models:\n",
    "    p_test = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    mets = metrics.metrics_basic(y_test, p_test)\n",
    "    aucs.append(mets['auc'])\n",
    "    loglosses.append(mets['logloss'])\n",
    "    briers.append(mets['brier'])\n",
    "    \n",
    "    ece, ece_table = metrics.expected_calibration_error(y_test, p_test, n_bins=15)\n",
    "    eces.append(ece)\n",
    "\n",
    "metrics.print_mean_std('AUC', aucs)\n",
    "metrics.print_mean_std('loglosses',loglosses)\n",
    "metrics.print_mean_std('briers',briers)\n",
    "metrics.print_mean_std('eces',eces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc1613-3cc3-492f-b480-a1502260b300",
   "metadata": {},
   "source": [
    "# Tuning history\n",
    "\n",
    "## Tune tree model params \n",
    "\"learning_rate\", \"num_leaves\", \"max_depth\", \"min_data_in_leaf\", \"lambda_l2\"\n",
    "and got the decent result below.\n",
    "\n",
    "## Tune features\n",
    "\n",
    "### Label-encode categorical features seed = 42:\n",
    "Test AUC    : 0.7182091248139506\n",
    "Test LogLoss: 0.5234532609680653\n",
    "\n",
    "\n",
    "### hashing categorical features seed = 42, <span style=\"color:red;\">Big Improvement!</span> \n",
    "\n",
    "Test AUC    : 0.7883122347769331\n",
    "Test LogLoss: 0.4587729242271498\n",
    "\n",
    "seed = 19\n",
    "Test AUC    : 0.7874018158198559\n",
    "Test LogLoss: 0.4593220177816222\n",
    "\n",
    "seed = 10\n",
    "Test AUC    : 0.7877994036835043\n",
    "Test LogLoss: 0.45899872089590293\n",
    "Best iter  : 620\n",
    "\n",
    "\n",
    "### nrows = 2000000 Train eval test split 0.8, 0.1, 0.1 seed = 10:\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": [\"auc\", \"binary_logloss\"],\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 255,\n",
    "    \"max_depth\": 12,\n",
    "    \"min_data_in_leaf\": 300,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"lambda_l2\": 10,\n",
    "    \"verbosity\": -1,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "Test\n",
    "{'auc': 0.7958963633226189, 'logloss': 0.450386281860432, 'pr_auc': 0.5856542748459541, 'brier': 0.14579051407211444, 'ctr_mean_true': 0.25125, 'ctr_mean_pred': 0.25023126129109174}\n",
    "ECE: 0.00551766077415308\n",
    "{'k': 0.01, 'n_top': 5000, 'ctr_top': 0.898, 'ctr_all': 0.25125, 'lift': 3.574129353233817}\n",
    "{'k': 0.05, 'n_top': 25000, 'ctr_top': 0.7814, 'ctr_all': 0.25125, 'lift': 3.110049751243769}\n",
    "\n",
    "### Same LightGBM params as above, nrows = 5000000 Train eval test split 0.8, 0.1, 0.1 seed = 11, 22, 33, get the mean and std for the metrics:\n",
    "\n",
    "AUC : 0.7961 ± 0.0002 \n",
    "loglosses: 0.4502 ± 0.0001 \n",
    "briers : 0.1457 ± 0.0000 \n",
    "eces : 0.0038 ± 0.0006\n",
    "\n",
    "### With log1p, freq and missing features added:\n",
    "\n",
    "AUC     : 0.7961 ± 0.0001\n",
    "loglosses: 0.4501 ± 0.0001\n",
    "briers  : 0.1457 ± 0.0000\n",
    "eces    : 0.0037 ± 0.0005\n",
    "\n",
    "{'auc': 0.7962079710668694, 'logloss': 0.4500324640233981, 'pr_auc': 0.5861662260191162, 'brier': 0.14565938497056644, 'ctr_mean_true': 0.25125, 'ctr_mean_pred': 0.2496694684545241}\n",
    "ECE: 0.00362046996990802\n",
    "{'k': 0.01, 'n_top': 5000, 'ctr_top': 0.8926, 'ctr_all': 0.25125, 'lift': 3.5526368159203843}\n",
    "{'k': 0.05, 'n_top': 25000, 'ctr_top': 0.78192, 'ctr_all': 0.25125, 'lift': 3.1121194029850625}\n",
    "\n",
    "### There was a bug above, missing features was called after fillna(), this is not intended, we fixed it! \n",
    "\n",
    "{'auc': 0.7964011491019027, 'logloss': 0.4498488910681045, 'pr_auc': 0.5866849981967813, 'brier': 0.14560254260769245, 'ctr_mean_true': 0.25125, 'ctr_mean_pred': 0.25001718486419217}\n",
    "ECE: 0.004156878282011214\n",
    "{'k': 0.01, 'n_top': 5000, 'ctr_top': 0.8986, 'ctr_all': 0.25125, 'lift': 3.5765174129353094}\n",
    "{'k': 0.05, 'n_top': 25000, 'ctr_top': 0.78296, 'ctr_all': 0.25125, 'lift': 3.1162587064676495}\n",
    "\n",
    "### Features above included along with feature crossing and log_freq, some dgradation  in performance observed:\n",
    "\n",
    "{'auc': 0.7946398988802232, 'logloss': 0.4517447045191126, 'pr_auc': 0.5831389395321789, 'brier': 0.14628356984096172, 'ctr_mean_true': 0.25125, 'ctr_mean_pred': 0.24875493089817768}\n",
    "ECE: 0.00886980180107436\n",
    "{'k': 0.01, 'n_top': 5000, 'ctr_top': 0.8912, 'ctr_all': 0.25125, 'lift': 3.547064676616902}\n",
    "{'k': 0.05, 'n_top': 25000, 'ctr_top': 0.77744, 'ctr_all': 0.25125, 'lift': 3.0942885572139183}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626ec2b-1d63-47bd-8bd2-20be00ed7600",
   "metadata": {},
   "source": [
    "# Shap analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f8e838f0-34e3-4fe1-aa63-7b77a49b7347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Riff/ctr-env/lib/python3.9/site-packages/shap/explainers/_tree.py:586: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "X_val_samples = X_val.sample(n=50*1000, replace=False, random_state=42)\n",
    "shap_values = explainer.shap_values(X_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4c18b499-10d7-4c9e-a79c-7b601f1c8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global summary\n",
    "shap.summary_plot(shap_values, X_val_samples, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../images/shap_summary.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0890effd-586d-4710-8c45-66d76c7acdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one row to explain (from val or test)\n",
    "i = 15\n",
    "x = X_val_samples.iloc[[i]]   # keep as DataFrame\n",
    "\n",
    "# SHAP contributions (log-odds space)\n",
    "contrib = model.predict(\n",
    "    x,\n",
    "    pred_contrib=True,\n",
    "    num_iteration=model.best_iteration\n",
    ")\n",
    "\n",
    "shap_values_single = contrib[0, :-1]   # per-feature contributions\n",
    "base_value_single  = contrib[0, -0]    # expected value (log-odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f7f4c0e4-2012-4f59-93fe-f8f72ae80cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = shap.Explanation(\n",
    "    values=shap_values_single,\n",
    "    base_values=base_value_single,\n",
    "    data=x.values[0],\n",
    "    feature_names=X_val.columns\n",
    ")\n",
    "\n",
    "shap.waterfall_plot(exp, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../images/shap_waterfall15.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea09098-70c2-489b-acab-51a5195e2c20",
   "metadata": {},
   "source": [
    "# SHAP analysis report\n",
    "\n",
    "## SHAP summary\n",
    "<div align=\"center\">\n",
    "  <img src=\"../images/shap_summary.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "### Top drivers are:\n",
    "\n",
    "- Numericals: I11, I6, I7\n",
    "- Categoricals: C14, C15, C17, C23, C13, C7\n",
    "\n",
    "### cross feature: C10xC17\n",
    "\n",
    "It appears very low in the ranking, near the bottom.\n",
    "\n",
    "That tells us:\n",
    "- The interaction exists\n",
    "- But it is weak relative to base features\n",
    "- It is not a dominant structural interaction\n",
    "- This matches the AUC drop — the model is not gaining meaningful signal from it.\n",
    "\n",
    "### Log-frequency features\n",
    "\n",
    "They are not among top global drivers.\n",
    "\n",
    "This is important:\n",
    "The model is not strongly using log(freq) features.\n",
    "So either:\n",
    "- Raw categorical IDs already capture the necessary signal\n",
    "- Frequency abstraction doesn’t add much at 5M scale\n",
    "- Or frequency features are too correlated with base IDs\n",
    "\n",
    "### Visual observation: slightly more noise spread\n",
    "\n",
    "The SHAP horizontal spread looks slightly wider for lower-ranked features.\n",
    "This suggests:\n",
    "- Added features increased model flexibility\n",
    "- But not necessarily improved generalization\n",
    "\n",
    "That’s consistent with:\n",
    "- Slight AUC drop\n",
    "- Worse ECE\n",
    "\n",
    "## Waterfall Plot (Single Example)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"../images/shap_waterfall15.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "\n",
    "### This is a high-CTR example (f(x) = 0.759).\n",
    "\n",
    "Baseline:\n",
    "E[f(X)] = -0.019\n",
    "\n",
    "- This baseline changed compared to earlier model — that’s a signal.\n",
    "\n",
    "### Dominant positive driver: C13 = 4916 (+0.49)\n",
    "\n",
    "Very strong ID effect.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- Raw categorical still dominates.\n",
    "\n",
    "- Cross feature did not replace this.\n",
    "\n",
    "### Negative drivers: I11, C14, I6\n",
    "\n",
    "Again:\n",
    "\n",
    "- Numericals matter strongly.\n",
    "\n",
    "- Cross feature not dominating.\n",
    "\n",
    "### Cross feature effect?\n",
    "\n",
    "C10xC17 does not appear in top local contributors.\n",
    "\n",
    "That confirms:\n",
    "\n",
    "- The cross is weak for this sample.\n",
    "\n",
    "## Big Picture Takeaways\n",
    "### Your baseline model was near the signal ceiling.\n",
    "\n",
    "At 5M rows, LightGBM already extracted most of the signal.\n",
    "### Cross features are not strong enough in this dataset.\n",
    "\n",
    "Not every intuitive cross helps.\n",
    "### Frequency abstraction does not dominate.\n",
    "\n",
    "### This suggests:\n",
    "- Either data already large enough\n",
    "- Or frequency signal overlaps with raw ID splits\n",
    "\n",
    "### SHAP validates metrics.\n",
    "\n",
    "- This is important:\n",
    "  - The visual interpretation matches the numeric regression.\n",
    "\n",
    "- That means:\n",
    "  - The experimental workflow is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637502f2-61e9-45b2-8758-4fd9f36e305d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
